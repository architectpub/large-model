|     |     |     |     |     |     |     |     |     |     |     |     |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Caregory | Tech component | Owner | is<br><br>Proprietary or Closed source | OSS License | comml<br><br>use | Model Szie(B) | Release Date | AP  | Source/<br><br>Paper | Star | Description |
| Multi-Model | ImageBind | Meta |     | [License](https://github.com/facebookresearch/ImageBind/blob/main/LICENSE) | No  |     |     |     | [Github](https://github.com/facebookresearch/ImageBind) | 5.9k | ImageBind One Embedding Space to Bind Them All |
| Image | DeepFloyd IF | [stability.ai](http://stability.ai/) |     | [License](https://github.com/deep-floyd/IF/blob/develop/LICENSE)<br><br>[Model license](https://github.com/deep-floyd/IF/blob/develop/LICENSE-MODEL) |     |     |     |     | [Github](https://github.com/deep-floyd/IF) | 6.4k | text-to-image model with a high degree of photorealism and language understanding |
| Stable Diffusion Version 2 | [stability.ai](http://stability.ai/) |     | MIT, unknown |     |     |     |     | [Github](https://github.com/Stability-AI/stablediffusion) | 23.5k | High-Resolution Image Synthesis with Latent Diffusion Models |
| DALL-E | OpenAI |     | Modified MIT | Yes |     |     |     | [Github](https://github.com/openai/DALL-E) | 10.3k | PyTorch package for the discrete VAE used for DALL·E. |
| DALL·E 2 | OpenAI | Yes |     |     |     |     |     | [product](https://openai.com/product/dall-e-2) |     |     |
| DALLE2-pytorch | lucidrains |     | MIT | Yes |     |     |     | [Github](https://github.com/lucidrains/DALLE2-pytorch) | 9.7k | Implementation of DALL-E 2, OpenAI's updated text-to-image synthesis neural network, in Pytorch |
| Speech | Whisper | OpenAI |     | MIT | Yes |     |     | R   | [Github](https://github.com/openai/whisper) | 37.7k | Robust Speech Recognition via Large-Scale Weak Supervision |
| MMS | Meta | Yes |     |     |     |     |     |     | [paper](https://about.fb.com/news/2023/05/ai-massively-multilingual-speech-technology/) |     |
| Code model | Codex | OpenAI | Yes |     |     | 12  | 2021/7/1 |     | [blog](https://openai.com/blog/openai-codex) | [Paper](https://arxiv.org/abs/2107.03374) |     |
| [AlphaCode](https://arxiv.org/abs/2203.07814v1) |     |     |     |     | 41  | Feb 2022 |     |     |     | Competition-Level Code Generation with AlphaCode |
| starcoder | BigCode | No  | Apache |     | 15  | May 2023 |     | [Github](https://github.com/bigcode-project/starcoder) | 4.8k | language model (LM) trained on source code and natural language text |
| CodeGen | Salesforce | No  | ?   |     |     |     |     | [Github](https://github.com/salesforce/CodeGen) | 3.6k | model for program synthesis. Trained on TPU-v4. Competitive with OpenAI Codex. |
| [Replit Code](https://huggingface.co/replit) | replit |     |     |     | 3   | May 2023 |     |     |     | replit-code-v1-3b model is a 2.7B LLM trained on 20 languages from the Stack Dedup v1.2 dataset. |
| [CodeGen2](https://arxiv.org/abs/2305.02309) | Salesforce |     | BSD | Yes | 1, 3, 7, 16 | May 2023 |     | [Github](https://github.com/salesforce/codegen2) |     | Code models for program synthesis. |
| [CodeT5 and CodeT5+](https://arxiv.org/abs/2305.07922) | Salesforce |     | BSD | Yes | 16  | May 2023 |     | [CodeT5](https://github.com/salesforce/codet5) |     | CodeT5 and CodeT5+ models for Code Understanding and Generation from Salesforce Research. |
| language model | [GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) |     |     |     |     |     | June 2018 |     | [GPT](https://github.com/openai/finetune-transformer-lm) |     | Improving Language Understanding by Generative Pre-Training |
| [BERT](https://arxiv.org/abs/1810.04805) |     |     |     |     |     | Oct 2018 |     | [BERT](https://github.com/google-research/bert) |     | Bidirectional Encoder Representations from Transformers |
| [RoBERTa](https://arxiv.org/abs/1907.11692) |     |     |     |     | 0.125 - 0.355 | July 2019 |     | [RoBERTa](https://github.com/facebookresearch/fairseq/tree/main/examples/roberta) |     | A Robustly Optimized BERT Pretraining Approach |
| [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf) |     |     |     |     | 1.5 | Nov 2019 |     | [GPT-2](https://github.com/openai/gpt-2) |     | Language Models are Unsupervised Multitask Learners |
| [T5](https://arxiv.org/abs/1910.10683) |     |     |     |     | 0.06 - 11 | Oct 2019 |     | [Flan-T5](https://github.com/google-research/t5x/blob/main/docs/models#flan-t5-checkpoints) |     | Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer |
| [XLNet](https://arxiv.org/abs/1906.08237) |     |     |     |     |     | Jun 2019 |     | [XLNet](https://github.com/zihangdai/xlnet#released-models) |     | Generalized Autoregressive Pretraining for Language Understanding and Generation |
| [ALBERT](https://arxiv.org/abs/1909.11942) |     |     |     |     | 0.235 | Sep 2019 |     | [ALBERT](https://github.com/google-research/ALBERT) |     | A Lite BERT for Self-supervised Learning of Language Representations |
| [CTRL](https://arxiv.org/abs/1909.05858) |     |     |     |     | 1.63 | Sep 2019 |     | [CTRL](https://github.com/salesforce/ctrl) |     | CTRL: A Conditional Transformer Language Model for Controllable Generation |
| GPT 3 | Azure | Yes |     |     | 175 | May 2020 | R   | [Paper](https://arxiv.org/abs/2005.14165) |     | Language Models are Few-Shot Learners |
| GShard |     |     |     |     | 600 | Jun 2020 |     | [Paper](http://arxiv.org/abs/2006.16668v1) |     | GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding |
| [BART](https://arxiv.org/abs/1910.13461) |     |     |     |     |     | Jul 2020 |     | [BART](https://github.com/facebookresearch/fairseq) |     | Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension |
| [mT5](https://arxiv.org/abs/2010.11934v3) |     |     |     |     | 13  | Oct 2020 |     | [mT5](https://github.com/google-research/multilingual-t5#released-model-checkpoints) |     | mT5: A massively multilingual pre-trained text-to-text transformer |
| [PanGu-α](https://arxiv.org/abs/2104.12369v1) |     |     |     |     | 13  | April 2021 |     | [PanGu-α](https://gitee.com/mindspore/models/tree/master/official/nlp/Pangu_alpha#download-the-checkpoint) |     | PanGu-α: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation |
| [CPM-2](https://arxiv.org/abs/2106.10715v3) |     |     |     |     | 198 | Jun 2021 |     | [CPM](https://github.com/TsinghuaAI/CPM) |     | CPM-2: Large-scale Cost-effective Pre-trained Language Models |
| GPT-J 6B | EleutherAI | No  |     | Yes | 6   | June 2021 |     | [GPT-J-6B](https://github.com/kingoflolz/mesh-transformer-jax/#gpt-j-6b) |     | A 6 billion parameter, autoregressive text generation model trained on The Pile. |
| ERNIE 3.0 | Baidu | Yes |     |     | 10  | July 2021 |     |     |     | ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation |
| [Jurassic-1](https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf) |     |     |     |     | 178 | Aug 2021 |     |     |     | Jurassic-1: Technical Details and Evaluation |
| [ERNIE 3.0 Titan](https://arxiv.org/abs/2112.12731v1) |     |     |     |     | 10  | July 2021 |     |     |     | ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation |
| [HyperCLOVA](https://arxiv.org/abs/2109.04650) |     |     |     |     | 82  | Sep 2021 |     |     |     | What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers |
| FLAN |     |     |     |     | 137 | 2021/10/1 |     | [Paper](https://arxiv.org/abs/2109.01652) |     | Finetuned Language Models Are Zero-Shot Learners |
| GPT 3.5 | Azure | Yes |     |     |     |     |     |     |     |     |
| GPT 4 | Azure | Yes |     |     |     | 2023/3/1 | R   |     |     |     |
| ERNIE 3.0 | Baidu | Yes |     |     | 10  | 2021/7/1 |     | [Paper](https://arxiv.org/abs/2107.02137) |     |     |
| Jurassic-1 |     |     |     |     | 178 | 2021/8/1 |     | [Paper](https://assets.website-files.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf) |     |     |
| [T0](https://arxiv.org/abs/2110.08207) |     |     |     |     | 11  | Oct 2021 |     | [T0](https://huggingface.co/bigscience/T0) |     | Multitask Prompted Training Enables Zero-Shot Task Generalization |
| [Yuan 1.0](https://arxiv.org/abs/2110.04725v2) |     |     |     |     | 245 | Oct 2021 |     |     |     | Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning |
| [WebGPT](https://arxiv.org/abs/2112.09332v3) |     |     |     |     | 175 | Dec 2021 |     |     |     | WebGPT: Browser-assisted question-answering with human feedback |
| [Gopher](https://arxiv.org/abs/2112.11446v2) |     |     |     |     | 280 | Dec 2021 |     |     |     | Scaling Language Models: Methods, Analysis & Insights from Training Gopher |
| [GLaM](https://arxiv.org/abs/2112.06905) |     |     |     |     | 1200 | Dec 2021 |     |     |     | GLaM: Efficient Scaling of Language Models with Mixture-of-Experts |
| LaMDA | Bard | Yes |     |     | 137 | Jan 2022 |     | [Paper](https://arxiv.org/abs/2201.08239) |     | LaMDA: Language Models for Dialog Applications |
| [MT-NLG](https://arxiv.org/abs/2201.11990v3) |     |     |     |     | 530 | Jan 2022 |     |     |     | Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model |
| [InstructGPT](https://arxiv.org/abs/2203.02155v1) |     |     |     |     | 175 | Mar 2022 |     |     |     | Training language models to follow instructions with human feedback |
| [Chinchilla](https://arxiv.org/abs/2203.15556) |     |     |     |     | 70  | Mar 2022 |     |     |     | Shows that for a compute budget, the best performances are not achieved by the largest models but by smaller models trained on more data. |
| [GPT-NeoX-20B](https://arxiv.org/abs/2204.06745v1) |     |     |     |     | 20  | April 2022 |     | [GPT-NeoX-20B](https://huggingface.co/EleutherAI/gpt-neox-20b) |     | GPT-NeoX-20B: An Open-Source Autoregressive Language Model |
| [Tk-Instruct](https://arxiv.org/abs/2204.07705v3) |     |     |     |     | 11  | April 2022 |     | [Tk-Instruct-11B](https://huggingface.co/allenai/tk-instruct-11b-def) |     | Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks |
| PALM | Google | Yes |     |     | 540 | April 2022 |     |     |     | PaLM: Scaling Language Modeling with Pathways |
| OPT | Meta | No  |     | Yes | 175 | May 2022 |     | [OPT-13B](https://huggingface.co/facebook/opt-13b), [OPT-66B](https://huggingface.co/facebook/opt-66b) ,[Paper](https://arxiv.org/abs/2205.01068) |     | OPT: Open Pre-trained Transformer Language Models |
| [OPT-IML](https://arxiv.org/abs/2212.12017v3) |     |     |     |     | 30, 175 | Dec 2022 |     | [OPT-IML](https://github.com/facebookresearch/metaseq/tree/main/projects/OPT-IML#pretrained-model-weights) |     | OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization |
| [GLM-130B](https://arxiv.org/abs/2210.02414v1) |     |     |     |     | 130 | Oct 2022 |     | [GLM-130B](https://github.com/THUDM/GLM-130B) |     | GLM-130B: An Open Bilingual Pre-trained Model |
| [AlexaTM](https://arxiv.org/abs/2208.01448v2) |     |     |     |     | 20  | Aug 2022 |     |     |     | AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model |
| [Flan-T5](https://arxiv.org/abs/2210.11416v5) |     |     |     |     | 11  | Oct 2022 |     | [Flan-T5-xxl](https://huggingface.co/google/flan-t5-xxl) |     | Scaling Instruction-Finetuned Language Models |
| [Sparrow](https://arxiv.org/abs/2209.14375) |     |     |     |     | 70  | Sep 2022 |     |     |     | Improving alignment of dialogue agents via targeted human judgements |
| [UL2](https://arxiv.org/abs/2205.05131v3) |     |     |     |     | 20  | Oct 2022 |     | [UL2, Flan-UL2](https://github.com/google-research/google-research/tree/master/ul2#checkpoints) |     | UL2: Unifying Language Learning Paradigms |
| [U-PaLM](https://arxiv.org/abs/2210.11399v2) |     |     |     |     | 540 | Oct 2022 |     |     |     | Transcending Scaling Laws with 0.1% Extra Compute |
| [BLOOM](https://arxiv.org/abs/2211.05100v3) | BigScience | Bo  |     | Yes | 176 | Nov 2022 |     | [BLOOM](https://huggingface.co/bigscience/bloom) ,[Paper](https://arxiv.org/abs/2211.01786) |     | BLOOM: A 176B-Parameter Open-Access Multilingual Language Model |
| [mT0](https://arxiv.org/abs/2211.01786v1) |     |     |     |     | 13  | Nov 2022 |     | [mT0-xxl](https://huggingface.co/bigscience/mt0-xxl) |     | Crosslingual Generalization through Multitask Finetuning |
| [Galactica](https://arxiv.org/abs/2211.09085v1) |     |     |     |     | 0.125 - 120 | Nov 2022 |     | [Galactica](https://huggingface.co/models?other=galactica) |     | Galactica: A Large Language Model for Science |
| [ChatGPT](https://openai.com/blog/chatgpt) |     |     |     |     |     | Nov 2022 |     |     |     | A model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests. |
| LLama | Meta | No  |     | No  | 7, 13, 33, 65 | 2023/2/1 |     | [Paper](https://arxiv.org/abs/2302.13971v1),[LLaMA](https://github.com/facebookresearch/llama) |     | LLaMA: Open and Efficient Foundation Language Models |
| [GPT-4](https://arxiv.org/abs/2303.08774v3) |     |     |     |     |     | March 2023 |     |     |     |     |
| PanGU-Σ |     | Yes |     |     | 1085 | 2023/3/1 |     |     |     | PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing |
| [BloombergGPT](https://arxiv.org/abs/2303.17564v1) |     |     |     |     | 50  | March 2023 |     |     |     | BloombergGPT: A Large Language Model for Finance |
| Cerebras-GPT | Cerebras | No  |     | Yes | 0.111 - 13 | 2023/3 |     | [hf](https://huggingface.co/cerebras) |     | Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster |
| oasst-sft-1-pythia-12b | LAION-AI | No  |     | Yes | 12  | 2023/3 | R   | [HF](https://huggingface.co/OpenAssistant) |     | OpenAssistant is a chat-based assistant that understands tasks, can interact with third-party systems, and retrieve information dynamically to do so. |
| Pythia | Eleuthera AI | No  |     | Yes | 0.070 - 12 | 2023/3 |     | [Pythia](https://github.com/eleutherai/pythia),<br><br>[Paper](https://arxiv.org/abs/2304.01373) |     | A suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. |
| StableLM |     | No  |     | No  | 3, 7 | April 2023 |     | [Github](https://github.com/Stability-AI/StableLM#stablelm-alpha) |     | Stability AI's StableLM series of language models |
| Dolly 2.0 | DataBricks | No  |     | Yes | 3, 7, 12 | 2023/4 | R   | [Dolly](https://huggingface.co/databricks/dolly-v2-12b) |     | An instruction-following LLM, fine-tuned on a human-generated instruction dataset licensed for research and commercial use. |
| [DLite](https://medium.com/ai-squared/announcing-dlite-v2-lightweight-open-llms-that-can-run-anywhere-a852e5978c6e) |     |     |     |     | 0.124 - 1.5 | 2023/5 |     | [HF](https://huggingface.co/aisquared/dlite-v2-1_5b) |     | Lightweight instruction following models which exhibit ChatGPT-like interactivity. |
| MPT-7B | MosaicML | No  | Apache 2 | Yes | 7   | 2023/5/5 | R   | [blog](https://www.mosaicml.com/blog/mpt-7b) |     | a GPT-style model, and the first in the MosaicML Foundation Series of models. |
| [h2oGPT](https://github.com/h2oai/h2ogpt) |     |     |     |     | 12  | 2023/5 |     | [HF](https://github.com/h2oai/h2ogpt) |     | h2oGPT is a large language model (LLM) fine-tuning framework and chatbot UI with document(s) question-answer capabilities. |
| [LIMA](https://arxiv.org/abs/2305.11206v1) |     |     |     |     | 65  | 2023/5 |     |     |     | A 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. |
| [RedPajama-INCITE](https://www.together.xyz/blog/redpajama-models-v1) |     |     |     |     | 3, 7 | 2023/5 |     | [HF](https://huggingface.co/togethercomputer) |     | A family of models including base, instruction-tuned & chat models. |
| [Gorilla](https://arxiv.org/abs/2305.15334v1) |     |     |     |     | 7   | 2023/5 |     | [Gorilla](https://github.com/ShishirPatil/gorilla) |     | Gorilla: Large Language Model Connected with Massive APIs |
| [Med-PaLM 2](https://arxiv.org/abs/2305.09617v1) |     |     |     |     |     | 2023/5 |     |     |     | Towards Expert-Level Medical Question Answering with Large Language Models |
| [PaLM 2](https://arxiv.org/abs/2305.10403) |     |     |     |     |     | 2023/5 |     |     |     | A Language Model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM. |
| [Falcon LLM](https://falconllm.tii.ae/) |     |     |     |     | 7, 40 | 2023/5 |     | [7B](https://huggingface.co/tiiuae), [40B](https://huggingface.co/tiiuae/falcon-40b) |     | foundational large language model (LLM) with 40 billion parameters trained on one trillion tokens. |
| Claude | Anthropic | Yes |     |     |     |     |     |     |     |     |
| GPT-Neo | Eleuthera AI | No  |     | Yes |     |     |     |     |     |     |
| GPT-Neox | Eleuthera AI | No  |     | Yes | 20  | 2022/2/1 |     | [Paper](https://arxiv.org/abs/2204.06745) |     |     |
| FastChat-T5-3B | LMSYS | No  | Apache | Yes |     | 2023/4/ | R   |     |     |     |
| OpenLLama | openlm-research | No  |     | Yes |     |     |     |     |     |     |
| OpenChatKit | Together | No  |     | Yes |     |     |     |     |     |     |
| YaLM | Yandex | No  |     | Yes | 100 | 2022/6/1 |     | [Github](https://github.com/yandex/YaLM-100B) |     |     |
| ChatGLM-6B | TsingHua | No  | ChatGLM-6B | No  | 6   | 2023/3/1 |     | [Github](https://github.com/THUDM/ChatGLM-6B) |     |     |
| Alpaca | Stanford | No  |     | No  |     |     |     |     |     |     |
| Vicuna |     | No  |     | No  | 13  | 2023/3/1 |     | [Blog](https://vicuna.lmsys.org/) |     |     |
| StableVicuna |     | No  |     | No  |     |     |     |     |     |     |
| RWKV-4-Raven-7B | BlinkDL | No  |     | No  |     |     |     |     |     |     |
| Alpaca-LoRA | tloen | No  |     | No  |     |     |     |     |     |     |
| Koala | BAIR | No  |     | No  | 13  | 2023/4/1 |     | [Blog](https://bair.berkeley.edu/blog/2023/04/03/koala/) |     |     |
